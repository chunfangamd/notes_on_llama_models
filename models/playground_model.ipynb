{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import MllamaVisionModel, FlaxMllamaVisionModel\n",
    "# from transformers import AutoProcessor, MllamaTextModel\n",
    "# import requests\n",
    "# import torch\n",
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "# from PIL import Image\n",
    "# from huggingface_hub import login\n",
    "# torch.set_printoptions(precision=8)\n",
    "# jax.numpy.set_printoptions(precision=10)\n",
    "# import os\n",
    "# os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.5'\n",
    "\n",
    "# hf_token = \"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\"\n",
    "# login(hf_token)\n",
    "# checkpoint = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "# processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "# url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "from PIL import Image\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"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\"\n",
    "# login(hf_token, add_to_git_credential=True)\n",
    "login(hf_token)\n",
    "\n",
    "checkpoint = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_torch = MllamaVisionModel.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "# import torch\n",
    "# # Paths to the files\n",
    "# model_path = \"/home/amd/chun/llama-models/downloaded_models/Llama3.2-11B-Vision-Instruct\"\n",
    "# tokenizer_path = f\"{model_path}/tokenizer.model\"\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# # Load model\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#     model_path,\n",
    "#     torch_dtype=torch.float16,  # Use float16 for reduced memory consumption (optional)\n",
    "#     device_map=\"auto\"          # Automatically map model layers to available devices\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_path = \"/home/amd/chun/llama-models/downloaded_models/Llama3.2-11B-Vision-Instruct/consolidated.00.pth\"\n",
    "model_weights = torch.load(model_path)\n",
    "\n",
    "# Check the model structure\n",
    "print(model_weights.keys())\n",
    "\n",
    "# # Save the keys to a file\n",
    "# with open('llama3_2_11B_vision_instruct_keys.txt', 'w') as f:\n",
    "#     for line in model_weights.keys():\n",
    "#         f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
